{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-11T23:21:28.460037Z","iopub.execute_input":"2022-06-11T23:21:28.460664Z","iopub.status.idle":"2022-06-11T23:21:28.467587Z","shell.execute_reply.started":"2022-06-11T23:21:28.460629Z","shell.execute_reply":"2022-06-11T23:21:28.466621Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"> # **MNIST Neural Network from Scratch w/ mult layers**\n\nObjetivo: Estudar o projeto criado de redes neurais somente com pandas e numpy para análise do dataset MNIST, melhorando o modelo com a adição de mais layers.","metadata":{}},{"cell_type":"markdown","source":"DIAGRAMA DO PROJETO: https://app.diagrams.net/#G1aNEniOxCATXECjdoEAyPIMMM3V8dTtxh","metadata":{}},{"cell_type":"markdown","source":"Estudo utilizando versão fonte: https://www.youtube.com/watch?v=w8yWXqWQYmU","metadata":{}},{"cell_type":"markdown","source":"Baseado no modelo de Samson Zhang: https://www.kaggle.com/code/wwsalmon/simple-mnist-nn-from-scratch-numpy-no-tf-keras/notebook","metadata":{}},{"cell_type":"markdown","source":"> **TRAZENDO OS DADOS DO DIGIT-RECOGNIZER**\n\n1. Importando os dados de treino","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\ndata = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\nsample = pd.read_csv('../input/digit-recognizer/sample_submission.csv')\n\nprint(data.head())\nprint(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:28.487314Z","iopub.execute_input":"2022-06-11T23:21:28.487665Z","iopub.status.idle":"2022-06-11T23:21:32.737423Z","shell.execute_reply.started":"2022-06-11T23:21:28.487636Z","shell.execute_reply":"2022-06-11T23:21:32.736343Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"'label' corresponde ao digito (0 - 9)\n\n'pixel' corresponde a escala em cinza (gray-scale) para cada pixel (0 - 784) onde referencia a imagem de 28w x 28h = 784","metadata":{}},{"cell_type":"markdown","source":"> # **INICIANDO A CRIAÇÃO DO MODELO MODELO**","metadata":{}},{"cell_type":"markdown","source":"> ### Preparando os dados","metadata":{}},{"cell_type":"markdown","source":"1. Transformando o dataset em um array com numpy\n\n2. Dimensão dos dados\n\n3. Para garantir que não ocorra **overfitting** baseado na distribuição dos dados, é utilizado a função random.shuffle() para reduzir o risco.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data = np.array(data)   # 1. Transforma o dataset em array\nm, n = data.shape       # 2. Dimensão dos dados (rows e features + 1(label))\nnp.random.shuffle(data) # 3. Elimina o risco de overfitting\nprint(\"Dados: \", data)\nprint(\"Rows: \", m)\nprint(\"Features + Label: \", n)\n\ntest = np.array(test)   # 1. Transforma o dataset em array\ntm, tn = test.shape       # 2. Dimensão dos dados (rows e features + 1(label))\nprint(\"Dados: \", data)\nprint(\"Rows: \", tm)\nprint(\"Features + Label: \", tn)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:32.739260Z","iopub.execute_input":"2022-06-11T23:21:32.739769Z","iopub.status.idle":"2022-06-11T23:21:33.455338Z","shell.execute_reply.started":"2022-06-11T23:21:32.739726Z","shell.execute_reply":"2022-06-11T23:21:33.454410Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"data_dev = data[0:1000].T #Transpõe a matriz\nY_dev = data_dev[0]       #Label -> primeira linha\nX_dev = data_dev[1:n]     #Features -> 784\nX_dev = X_dev / 255.\n\ndata_train = data[1000:m].T #Transpõe a matriz\nY_train = data_train[0]     #Label -> primeira linha\nX_train = data_train[1:n]   #Features -> 784\nX_train = X_train / 255.\n_,m_train = X_train.shape\n\n\ndata_test = test[0:tm].T #Transpõe a matriz\nX_test = data_test[0:tn]   #Features -> 784\nX_test = X_test / 255.\n_,m_test = X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:33.456702Z","iopub.execute_input":"2022-06-11T23:21:33.457619Z","iopub.status.idle":"2022-06-11T23:21:33.662596Z","shell.execute_reply.started":"2022-06-11T23:21:33.457569Z","shell.execute_reply":"2022-06-11T23:21:33.661892Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"> # INICIANDO MODELO DE PROPAGAÇÃO","metadata":{}},{"cell_type":"markdown","source":"> ### Funções iniciais e de ativação","metadata":{}},{"cell_type":"markdown","source":"1. init_params() -> Cria dados em formato de uma distribuição normal uniforme entre -0,5 e 0,5 para os parametros de W = Weights e b = Bias para os Nodes ( 1 e 2 )\n\n2. ReLU(Z) -> Função de ativação que transforma Z no método ReLU (if x>0 them x else 0)\n\n3. Softmax(Z) -> Função de ativação no método softmax\n\n4. one_hot(Y) -> Transforma os valores de um array Y em uma matriz para 0 ou 1 (one-hot-econding) para cada valor.","metadata":{}},{"cell_type":"code","source":"# A função np.random.rand retorna valores entre -1 e 1, \n# então adicionamos o -0,5 para retornar valores entre -0,5 e 0,5\ndef init_params(): \n    W1 = np.random.rand(200, 784) - 0.5 \n    b1 = np.random.rand(200, 1) - 0.5\n    W2 = np.random.rand(10, 200) - 0.5\n    b2 = np.random.rand(10, 1) - 0.5\n    return W1, b1, W2, b2\n\ndef ReLU(Z):\n    return np.maximum(Z, 0) # Para cada valor de Z, se for maior que 0 retorna Z, se não, retorna 0.\n\ndef softmax(Z):\n    A = np.exp(Z) / sum(np.exp(Z)) # Função exponencial, então temos e^Z / sum(e^Z)\n    return A\n\ndef ReLU_deriv(Z): # Se ReLU é linear, sua derivada é 0 então:\n    return Z > 0 # true = 1 false = 0\n\ndef one_hot(Y):\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    one_hot_Y = one_hot_Y.T\n    return one_hot_Y","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:33.664954Z","iopub.execute_input":"2022-06-11T23:21:33.665548Z","iopub.status.idle":"2022-06-11T23:21:33.675304Z","shell.execute_reply.started":"2022-06-11T23:21:33.665506Z","shell.execute_reply":"2022-06-11T23:21:33.674596Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"> ### Foward Propagation","metadata":{}},{"cell_type":"markdown","source":"OBS: Baseado no diagrama do método de propagação, X = A0 -> input layer\n     .dot() = Dot Product, referente ao produto escalar de matrizes","metadata":{}},{"cell_type":"code","source":"def forward_prop(W1, b1, W2, b2, X):\n    Z1 = W1.dot(X) + b1 # Produto escalar W1 e X(input layer)\n    A1 = ReLU(Z1)\n    Z2 = W2.dot(A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:33.676588Z","iopub.execute_input":"2022-06-11T23:21:33.677053Z","iopub.status.idle":"2022-06-11T23:21:33.685917Z","shell.execute_reply.started":"2022-06-11T23:21:33.677023Z","shell.execute_reply":"2022-06-11T23:21:33.685285Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"> ### Backward Propagation","metadata":{}},{"cell_type":"code","source":"def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n    one_hot_Y = one_hot(Y)\n    dZ2 = A2 - one_hot_Y\n    dW2 = 1 / m * dZ2.dot(A1.T)\n    db2 = 1 / m * np.sum(dZ2)\n    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T)\n    db1 = 1 / m * np.sum(dZ1)\n    return dW1, db1, dW2, db2","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:33.687521Z","iopub.execute_input":"2022-06-11T23:21:33.687908Z","iopub.status.idle":"2022-06-11T23:21:33.696517Z","shell.execute_reply.started":"2022-06-11T23:21:33.687869Z","shell.execute_reply":"2022-06-11T23:21:33.695741Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"> ### Update Parametros","metadata":{}},{"cell_type":"markdown","source":"Encontra o melhor estimador para Weights e Bias","metadata":{}},{"cell_type":"code","source":"def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1    \n    W2 = W2 - alpha * dW2  \n    b2 = b2 - alpha * db2    \n    return W1, b1, W2, b2","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:33.697482Z","iopub.execute_input":"2022-06-11T23:21:33.698000Z","iopub.status.idle":"2022-06-11T23:21:33.709710Z","shell.execute_reply.started":"2022-06-11T23:21:33.697969Z","shell.execute_reply":"2022-06-11T23:21:33.708834Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"> ### Funções para treinar o modelo","metadata":{}},{"cell_type":"markdown","source":"1. get_predictions(A2) -> retorna o maior valor do indice de um np.array\n\n2. get_accuracy(predictions, Y) -> soma(np.argmax(A2, 0) == Y) / Y.size\n\n3. gradient_descent(X, Y, alpha, iterations) -> treina o modelo com dados de treino, alpha = learning rate e iterations é o numero de ciclos de aprendizagem do modelo","metadata":{}},{"cell_type":"code","source":"def get_predictions(A2):\n    return np.argmax(A2, 0)\n\ndef get_accuracy(predictions, Y):\n    print(predictions, Y)\n    return np.sum(predictions == Y) / Y.size\n\ndef gradient_descent(X, Y, alpha, iterations):\n    W1, b1, W2, b2 = init_params() # Cria parametros de Weight e Bias\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 10 == 0: # Para cada 10 iterations, print\n            print(\"Iteration: \", i)\n            predictions = get_predictions(A2)\n            print(get_accuracy(predictions, Y))\n    return W1, b1, W2, b2","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:33.710624Z","iopub.execute_input":"2022-06-11T23:21:33.711424Z","iopub.status.idle":"2022-06-11T23:21:33.720515Z","shell.execute_reply.started":"2022-06-11T23:21:33.711389Z","shell.execute_reply":"2022-06-11T23:21:33.719729Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"> #### Executando o modelo","metadata":{}},{"cell_type":"code","source":"W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 10)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:33.721614Z","iopub.execute_input":"2022-06-11T23:21:33.722301Z","iopub.status.idle":"2022-06-11T23:21:40.717822Z","shell.execute_reply.started":"2022-06-11T23:21:33.722267Z","shell.execute_reply":"2022-06-11T23:21:40.716729Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"> ### Funções para executar o modelo","metadata":{}},{"cell_type":"code","source":"def make_predictions(X, W1, b1, W2, b2):\n    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n    predictions = get_predictions(A2)\n    return predictions\n\ndef test_prediction(index, W1, b1, W2, b2):\n    current_image = X_train[:, index, None]\n    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n    label = Y_train[index]\n    print(\"Prediction: \", prediction)\n    print(\"Label: \", label)\n    \n    current_image = current_image.reshape((28, 28)) * 255\n    plt.gray()\n    plt.imshow(current_image, interpolation='nearest')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:40.721141Z","iopub.execute_input":"2022-06-11T23:21:40.721820Z","iopub.status.idle":"2022-06-11T23:21:40.732795Z","shell.execute_reply.started":"2022-06-11T23:21:40.721773Z","shell.execute_reply":"2022-06-11T23:21:40.731884Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"> ### Predição com o modelo","metadata":{}},{"cell_type":"code","source":"indexrand = np.random.randint(1,tm)\ntest_prediction(indexrand, W1, b1, W2, b2)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:40.734498Z","iopub.execute_input":"2022-06-11T23:21:40.735185Z","iopub.status.idle":"2022-06-11T23:21:40.954790Z","shell.execute_reply.started":"2022-06-11T23:21:40.735133Z","shell.execute_reply":"2022-06-11T23:21:40.953932Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\nget_accuracy(dev_predictions, Y_dev)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:40.956121Z","iopub.execute_input":"2022-06-11T23:21:40.956584Z","iopub.status.idle":"2022-06-11T23:21:40.989163Z","shell.execute_reply.started":"2022-06-11T23:21:40.956538Z","shell.execute_reply":"2022-06-11T23:21:40.988295Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"> ### Dados de test.csv","metadata":{}},{"cell_type":"markdown","source":"Testando a acurácia do modelo com os dados de test.csv","metadata":{}},{"cell_type":"code","source":"test_predictions = make_predictions(X_test, W1, b1, W2, b2)\nj = test_predictions.shape\n\nprint(\"Predictions: \",test_predictions, \"rows: \", j[0])\n#get_accuracy(dev_predictions, Y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:40.990542Z","iopub.execute_input":"2022-06-11T23:21:40.991109Z","iopub.status.idle":"2022-06-11T23:21:41.272310Z","shell.execute_reply.started":"2022-06-11T23:21:40.991067Z","shell.execute_reply":"2022-06-11T23:21:41.271315Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"submissions = pd.DataFrame({\"ImageId\": list(range(1,len(test_predictions)+1)),\n    \"Label\": test_predictions})\nsubmissions.to_csv(\"submissions.csv\", index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:41.277360Z","iopub.execute_input":"2022-06-11T23:21:41.280168Z","iopub.status.idle":"2022-06-11T23:21:41.384255Z","shell.execute_reply.started":"2022-06-11T23:21:41.280116Z","shell.execute_reply":"2022-06-11T23:21:41.383595Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# TENTANDO CRIAR UM ALGORITMO DE DEEP LEANING","metadata":{}},{"cell_type":"code","source":"#classe de redes neurais\nclass neural_network:\n    \n    def init_params(nodes, X):\n        \n        #Numero de Features para automatizar input layers\n        X = np.array(X)\n        m, n = X.shape\n        \n        #Criando W e b baseado na quantidade de nodes e layers\n        W1 = np.random.rand(nodes, m) - 0.5 \n        b1 = np.random.rand(nodes, 1) - 0.5\n        W2 = np.random.rand(10, nodes) - 0.5\n        b2 = np.random.rand(10, 1) - 0.5\n        return W1, b1, W2, b2\n    \n    def ReLU(Z):\n        # Para cada valor de Z, se for maior que 0 retorna Z, se não, retorna 0.\n        return np.maximum(Z, 0) \n\n    def softmax(Z):\n        # Função exponencial, então temos e^Z / sum(e^Z)\n        A = np.exp(Z) / sum(np.exp(Z)) \n        return A\n\n    # Se ReLU é linear, sua derivada é 0 então:\n    def ReLU_deriv(Z): \n        # true = 1 false = 0\n        return Z > 0 \n\n    def one_hot(Y):\n        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n        one_hot_Y[np.arange(Y.size), Y] = 1\n        one_hot_Y = one_hot_Y.T\n        return one_hot_Y\n    \n    def forward_prop(W1, b1, W2, b2, X):\n        Z1 = W1.dot(X) + b1 # Produto escalar W1 e X(input layer)\n        A1 = neural_network.ReLU(Z1)\n        Z2 = W2.dot(A1) + b2\n        A2 = neural_network.softmax(Z2)\n        return Z1, A1, Z2, A2\n\n    def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n        one_hot_Y = neural_network.one_hot(Y)\n        dZ2 = A2 - one_hot_Y\n        dW2 = 1 / m * dZ2.dot(A1.T)\n        db2 = 1 / m * np.sum(dZ2)\n        dZ1 = W2.T.dot(dZ2) * neural_network.ReLU_deriv(Z1)\n        dW1 = 1 / m * dZ1.dot(X.T)\n        db1 = 1 / m * np.sum(dZ1)\n        return dW1, db1, dW2, db2\n\n    def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n        W1 = W1 - alpha * dW1\n        b1 = b1 - alpha * db1    \n        W2 = W2 - alpha * dW2  \n        b2 = b2 - alpha * db2    \n        return W1, b1, W2, b2\n    \n    def get_predictions(A2):\n        return np.argmax(A2, 0)\n    \n    def predict(X, model):\n        _, _, _, A2 = neural_network.forward_prop(model[0], model[1], model[2], model[3], X)\n        predictions = neural_network.get_predictions(A2)\n        return predictions\n    \n    def get_accuracy(predictions, Y):\n        return np.sum(predictions == Y) / Y.size\n    \n    def train(X, Y, alpha, iterations, nodes):\n        W1, b1, W2, b2 = neural_network.init_params(nodes ,X) # Cria parametros de Weight e Bias\n        for i in range(iterations):\n            Z1, A1, Z2, A2 = neural_network.forward_prop(W1, b1, W2, b2, X)\n            dW1, db1, dW2, db2 = neural_network.backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n            W1, b1, W2, b2 = neural_network.update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n            if i % 10 == 0: # Para cada 10 iterations, print\n                print(\"Iteration: \", i)\n                predictions = neural_network.get_predictions(A2)\n                print(neural_network.get_accuracy(predictions, Y))\n        return W1, b1, W2, b2","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:41.386677Z","iopub.execute_input":"2022-06-11T23:21:41.387091Z","iopub.status.idle":"2022-06-11T23:21:41.408587Z","shell.execute_reply.started":"2022-06-11T23:21:41.387050Z","shell.execute_reply":"2022-06-11T23:21:41.407667Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model = neural_network.train(X_train, Y_train, 0.1, 10, 10)\npredicao = neural_network.predict(X_train,model)\nneural_network.get_accuracy(predicao, Y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:21:41.409736Z","iopub.execute_input":"2022-06-11T23:21:41.410037Z","iopub.status.idle":"2022-06-11T23:21:42.944780Z","shell.execute_reply.started":"2022-06-11T23:21:41.410009Z","shell.execute_reply":"2022-06-11T23:21:42.943904Z"},"trusted":true},"execution_count":35,"outputs":[]}]}