{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    \n",
    "    def init_params(nodes, X):\n",
    "        \n",
    "        X = np.array(X)\n",
    "        m, n = X.shape\n",
    "        \n",
    "        W1 = np.random.rand(nodes, m) - 0.5 \n",
    "        b1 = np.random.rand(nodes, 1) - 0.5\n",
    "        W2 = np.random.rand(10, nodes) - 0.5\n",
    "        b2 = np.random.rand(10, 1) - 0.5\n",
    "        return W1, b1, W2, b2\n",
    "    \n",
    "    def ReLU(Z):\n",
    "        return np.maximum(Z, 0) # Para cada valor de Z, se for maior que 0 retorna Z, se não, retorna 0.\n",
    "\n",
    "    def softmax(Z):\n",
    "        A = np.exp(Z) / sum(np.exp(Z)) # Função exponencial, então temos e^Z / sum(e^Z)\n",
    "        return A\n",
    "\n",
    "    def ReLU_deriv(Z): # Se ReLU é linear, sua derivada é 0 então:\n",
    "        return Z > 0 # true = 1 false = 0\n",
    "\n",
    "    def one_hot(Y):\n",
    "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "        one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "        one_hot_Y = one_hot_Y.T\n",
    "        return one_hot_Y\n",
    "    \n",
    "    def forward_prop(W1, b1, W2, b2, X):\n",
    "        Z1 = W1.dot(X) + b1 # Produto escalar W1 e X(input layer)\n",
    "        A1 = neural_network.ReLU(Z1)\n",
    "        Z2 = W2.dot(A1) + b2\n",
    "        A2 = neural_network.softmax(Z2)\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "        one_hot_Y = neural_network.one_hot(Y)\n",
    "        dZ2 = A2 - one_hot_Y\n",
    "        dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "        db2 = 1 / m * np.sum(dZ2)\n",
    "        dZ1 = W2.T.dot(dZ2) * neural_network.ReLU_deriv(Z1)\n",
    "        dW1 = 1 / m * dZ1.dot(X.T)\n",
    "        db1 = 1 / m * np.sum(dZ1)\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "        W1 = W1 - alpha * dW1\n",
    "        b1 = b1 - alpha * db1    \n",
    "        W2 = W2 - alpha * dW2  \n",
    "        b2 = b2 - alpha * db2    \n",
    "        return W1, b1, W2, b2\n",
    "    \n",
    "    def get_predictions(A2):\n",
    "        return np.argmax(A2, 0)\n",
    "    \n",
    "    def predict(X, model):\n",
    "        _, _, _, A2 = neural_network.forward_prop(model[0], model[1], model[2], model[3], X)\n",
    "        predictions = neural_network.get_predictions(A2)\n",
    "        return predictions\n",
    "    \n",
    "    def get_accuracy(predictions, Y):\n",
    "        return np.sum(predictions == Y) / Y.size\n",
    "    \n",
    "    def train(X, Y, alpha, iterations, nodes):\n",
    "        W1, b1, W2, b2 = neural_network.init_params(nodes ,X) # Cria parametros de Weight e Bias\n",
    "        for i in range(iterations):\n",
    "            Z1, A1, Z2, A2 = neural_network.forward_prop(W1, b1, W2, b2, X)\n",
    "            dW1, db1, dW2, db2 = neural_network.backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "            W1, b1, W2, b2 = neural_network.update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "            if i % 10 == 0: # Para cada 10 iterations, print\n",
    "                print(\"Iteration: \", i)\n",
    "                predictions = neural_network.get_predictions(A2)\n",
    "                print(neural_network.get_accuracy(predictions, Y))\n",
    "        return W1, b1, W2, b2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
